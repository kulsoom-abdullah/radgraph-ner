{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import config\n",
    "import logging\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Insights and Comparison for RadGraph\n",
    "\n",
    "This notebook explores how the RadGraph corpus is tokenized by:\n",
    "- Custom tokenizers (BPE and WordPiece) trained from scratch.\n",
    "- Pretrained tokenizers used in popular biomedical language models (e.g., Bio_ClinicalBERT).\n",
    "\n",
    "### Objectives:\n",
    "1. Analyze the RadGraph corpus to understand token distributions.\n",
    "2. Compare tokenization behavior for common, rare, and domain-specific terms.\n",
    "3. Highlight potential challenges with subword splits in pretrained tokenizers.\n",
    "4. Draw insights on whether custom tokenization offers benefits for downstream tasks.\n",
    "\n",
    "**Note**: This step is **not required** for training the Named Entity Recognition (NER) model. It is only included here to facilitate experimentation and understanding of how domain-specific tokenization works when training from scratch using RadGraph data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load reports\n",
    "def load_reports(file_path):\n",
    "    \"\"\"Load reports from a JSONL file.\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# Count total tokens and unique tokens\n",
    "def count_tokens_by_space(input_file):\n",
    "    \"\"\"Count the total number of space-separated tokens in the dataset and track unique tokens.\"\"\"\n",
    "    reports = load_reports(input_file) \n",
    "    total_tokens = 0\n",
    "    tokens = set()  # Use a set to track unique tokens\n",
    "    for report in reports:\n",
    "        if isinstance(report, str):\n",
    "            total_tokens += len(report.split())\n",
    "            tokens.update(report.split())\n",
    "        elif isinstance(report, dict) and \"text\" in report:\n",
    "            total_tokens += len(report[\"text\"].split())\n",
    "            tokens.update(report[\"text\"].split())\n",
    "    return total_tokens, tokens\n",
    "\n",
    "# Count occurrences of tokens across entities\n",
    "def count_token_occurrences(input_file):\n",
    "    \"\"\"Count occurrences of individual token text (words/phrases) across all entities.\"\"\"\n",
    "    reports = load_reports(input_file)\n",
    "    token_counter = Counter()\n",
    "    for report in reports:\n",
    "        for label_data in report[\"labels\"]:\n",
    "            token_counter[label_data[\"tokens\"]] += 1  # Increment count for each token text\n",
    "    return token_counter\n",
    "\n",
    "# Extract entity text and labels\n",
    "def extract_entity_text_and_labels_with_counts(input_file):\n",
    "    \"\"\"Extract entity text, labels, and their counts.\"\"\"\n",
    "    reports = load_reports(input_file)\n",
    "    entity_text_and_labels = []\n",
    "    entity_counter = defaultdict(int)  # Count occurrences of each entity label\n",
    "\n",
    "    for report in reports:\n",
    "        for label_data in report[\"labels\"]:\n",
    "            entity_text_and_labels.append({\n",
    "                \"text\": label_data[\"tokens\"],  # Entity text\n",
    "                \"label\": label_data[\"label\"]  # Entity label\n",
    "            })\n",
    "            entity_counter[label_data[\"label\"]] += 1  # Increment the label count\n",
    "\n",
    "    return entity_text_and_labels, entity_counter\n",
    "\n",
    "# Input file\n",
    "input_file = \"../data/processed/radgraph.jsonl\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: RadGraph Corpus Insights\n",
    "\n",
    "In this section, I analyze the RadGraph corpus to understand the distribution of tokens and their frequencies.\n",
    "\n",
    "### **Token Counts**\n",
    "- **Total Tokens**: 15,815,652\n",
    "- **Unique Tokens**: 33,855\n",
    "- The significant difference between total and unique tokens highlights the repetitive nature of the dataset, with frequent mentions of anatomical and diagnostic terms.\n",
    "\n",
    "### **Token Frequency**\n",
    "- **Top Tokens**:\n",
    "  - Common terms include \"RIGHT (133,060 occurrences),\" \"LEFT (127,717),\" and \"PLEURAL (115,285).\"\n",
    "- **Middle Tokens**:\n",
    "  - Domain-specific phrases like \"hyper - inflated\" and \"tumoral infiltration\" appear twice.\n",
    "- **Bottom Tokens**:\n",
    "  - Rare terms like \"FOCAL BULLAE\" and \"NEOPLASTIC FOCUS\" appear only once.\n",
    "\n",
    "### **Vocabulary Size Considerations**\n",
    "- RadGraph has **33,855 unique tokens**, aligning closely with vocabulary sizes used in pretrained models like:\n",
    "  - **Bio_ClinicalBERT** (~30,000 tokens)\n",
    "  - **PubMedBERT** (~30,000 tokens)\n",
    "- A vocabulary size of **30,000 tokens** is reasonable for RadGraph:\n",
    "  - It closely matches the corpus’ unique token count, minimizing token splits.\n",
    "  - Larger vocabularies (e.g., 50,000 tokens) would increase computational overhead without significant benefits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of space-separated tokens in the corpus: 15815652\n",
      "Total unique space-separated tokens in the corpus: 33855\n"
     ]
    }
   ],
   "source": [
    "# Count tokens\n",
    "total_tokens, tokens = count_tokens_by_space(input_file)\n",
    "print(f\"Total number of space-separated tokens in the corpus: {total_tokens}\")\n",
    "print(f\"Total unique space-separated tokens in the corpus: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top tokens by count:\n",
      "RIGHT: 133060\n",
      "LEFT: 127717\n",
      "PLEURAL: 115285\n",
      "STABLE: 85669\n",
      "EFFUSION: 84829\n",
      "LUNG: 83813\n",
      "PULMONARY: 83238\n",
      "right: 78237\n",
      "EDEMA: 74638\n",
      "left: 71237\n",
      "PNEUMOTHORAX: 66651\n",
      "pleural: 65192\n",
      "UNCHANGED: 60725\n",
      "ATELECTASIS: 58278\n",
      "CONSOLIDATION: 53031\n",
      "\n",
      "Middle tokens by count:\n",
      "MOST PROMINENTLY: 2\n",
      "VARIABLE WAXING AND WANING: 2\n",
      "TUMORAL INFILTRATION: 2\n",
      "MARKEDLY REDUCED: 2\n",
      "hyper - inflated: 2\n",
      "\n",
      "Bottom tokens by count:\n",
      "FOCAL BULLAE: 1\n",
      "RIGHT ASCENDING AORTA: 1\n",
      "DISSECTING AIR: 1\n",
      "ACHALASIA: 1\n",
      "NEOPLASTIC FOCUS: 1\n"
     ]
    }
   ],
   "source": [
    "# Count token occurrences\n",
    "token_counts = count_token_occurrences(input_file)\n",
    "\n",
    "# Sort tokens by count (descending)\n",
    "sorted_tokens_by_count = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display token statistics\n",
    "print(\"Top tokens by count:\")\n",
    "for token, count in sorted_tokens_by_count[:15]:  # Top 5\n",
    "    print(f\"{token}: {count}\")\n",
    "\n",
    "print(\"\\nMiddle tokens by count:\")\n",
    "middle_index = len(sorted_tokens_by_count) // 2\n",
    "for token, count in sorted_tokens_by_count[middle_index:middle_index + 5]:  # 5 around the middle\n",
    "    print(f\"{token}: {count}\")\n",
    "\n",
    "print(\"\\nBottom tokens by count:\")\n",
    "for token, count in sorted_tokens_by_count[-5:]:  # Bottom 5\n",
    "    print(f\"{token}: {count}\")\n",
    "\n",
    "# Sample top, middle, and bottom tokens\n",
    "top_tokens = [token for token, _ in sorted_tokens_by_count[:15]]  # Top 5 tokens\n",
    "middle_tokens = [token for token, _ in sorted_tokens_by_count[middle_index:middle_index + 5]]  # Middle 5 tokens\n",
    "bottom_tokens = [token for token, _ in sorted_tokens_by_count[-5:]]  # Bottom 5 tokens\n",
    "\n",
    "# Combine sampled tokens for tokenization comparison\n",
    "sampled_tokens = top_tokens + middle_tokens + bottom_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Tokenization Comparison\n",
    "\n",
    "This section compares how sampled tokens from RadGraph are tokenized by:\n",
    "- BPE and WordPiece (trained from scratch).\n",
    "- Pretrained tokenizers in popular biomedical models (e.g., Bio_ClinicalBERT, ClinicalBERT).\n",
    "\n",
    "### **Comparison Table**\n",
    "The table below shows tokenization results for:\n",
    "- **Top 15 frequent tokens**\n",
    "- **Middle 5 tokens**\n",
    "- **Bottom 5 rare tokens**\n",
    "\n",
    "| Token                      |   Count | BPE                        | WordPiece                  | DistilBERT                       | Bio_ClinicalBERT                 | MedBERT                                         | ClinicalBERT                        | SapBERT                          | BioMistral                           | BiomedCLIP                       |\n",
    "|:---------------------------|--------:|:---------------------------|:---------------------------|:---------------------------------|:---------------------------------|:------------------------------------------------|:------------------------------------|:---------------------------------|:-------------------------------------|:---------------------------------|\n",
    "| RIGHT                      |  133060 | RIGHT                      | RIGHT                      | right                            | right                            | R ##IG ##HT                                     | right                               | right                            | ▁R IGHT                              | right                            |\n",
    "| LEFT                       |  127717 | LEFT                       | LEFT                       | left                             | left                             | L ##EF ##T                                      | left                                | left                             | ▁LE FT                               | left                             |\n",
    "| PLEURAL                    |  115285 | PLEURAL                    | PLEURAL                    | pl ##eur ##al                    | p ##le ##ural                    | P ##LE ##UR ##AL                                | pl ##eur ##al                       | pleural                          | ▁P LE UR AL                          | pleural                          |\n",
    "| STABLE                     |   85669 | STABLE                     | STABLE                     | stable                           | stable                           | ST ##AB ##LE                                    | stable                              | stable                           | ▁ST ABLE                             | stable                           |\n",
    "| EFFUSION                   |   84829 | EFFUSION                   | EFFUSION                   | e ##ff ##usion                   | e ##ff ##usion                   | E ##FF ##US ##ION                               | ef ##fus ##ion                      | effusion                         | ▁E FF US ION                         | effusion                         |\n",
    "| LUNG                       |   83813 | LUNG                       | LUNG                       | lung                             | lung                             | L ##UN ##G                                      | lung                                | lung                             | ▁L UN G                              | lung                             |\n",
    "| PULMONARY                  |   83238 | PULMONARY                  | PULMONARY                  | pulmonary                        | pulmonary                        | P ##U ##LM ##ON ##AR ##Y                        | pu ##lm ##onar ##y                  | pulmonary                        | ▁P UL MON ARY                        | pulmonary                        |\n",
    "| right                      |   78237 | right                      | right                      | right                            | right                            | right                                           | right                               | right                            | ▁right                               | right                            |\n",
    "| EDEMA                      |   74638 | EDEMA                      | EDEMA                      | ed ##ema                         | ed ##ema                         | E ##DE ##MA                                     | ede ##ma                            | edema                            | ▁E DE MA                             | edema                            |\n",
    "| left                       |   71237 | left                       | left                       | left                             | left                             | left                                            | left                                | left                             | ▁left                                | left                             |\n",
    "| PNEUMOTHORAX               |   66651 | PNEUMOTHORAX               | PNEUMOTHORAX               | p ##ne ##um ##otho ##ra ##x      | p ##ne ##um ##oth ##orax         | P ##NE ##UM ##OT ##H ##OR ##A ##X               | pne ##umo ##th ##orax               | pneumothorax                     | ▁P NE UM OT H OR AX                  | pneumothorax                     |\n",
    "| pleural                    |   65192 | pleural                    | pleural                    | pl ##eur ##al                    | p ##le ##ural                    | p ##le ##ural                                   | pl ##eur ##al                       | pleural                          | ▁ple ural                            | pleural                          |\n",
    "| UNCHANGED                  |   60725 | UNCHANGED                  | UNCHANGED                  | unchanged                        | unchanged                        | UN ##CH ##AN ##GE ##D                           | un ##chang ##ed                     | unchanged                        | ▁UN CHAN G ED                        | unchanged                        |\n",
    "| ATELECTASIS                |   58278 | ATELECTASIS                | ATELECTASIS                | ate ##le ##cta ##sis             | ate ##lect ##asis                | AT ##EL ##EC ##TA ##SI ##S                      | at ##ele ##cta ##sis                | ate ##lec ##ta ##sis             | ▁A TE LECT AS IS                     | ate ##le ##ct ##asis             |\n",
    "| CONSOLIDATION              |   53031 | CONSOLIDATION              | CONSOLIDATION              | consolidation                    | consolidation                    | CO ##NS ##OL ##ID ##AT ##ION                    | con ##sol ##idat ##ion              | consolidation                    | ▁CON S OL ID ATION                   | consolidation                    |\n",
    "| MOST PROMINENTLY           |       2 | MOST PROMINENTLY           | MOST PROMINENTLY           | most prominently                 | most prominently                 | M ##OS ##T PR ##OM ##IN ##EN ##TL ##Y           | most prominent ##ly                 | most prominently                 | ▁MO ST ▁P ROM IN ENT LY              | most prominently                 |\n",
    "| VARIABLE WAXING AND WANING |       2 | VARIABLE WAXING AND WANING | VARIABLE WAXING AND WANING | variable wax ##ing and wan ##ing | variable wax ##ing and wa ##ning | VA ##RI ##AB ##LE WA ##X ##ING AND WA ##NI ##NG | variable wa ##xin ##g and wa ##ning | variable wax ##ing and wan ##ing | ▁VAR I ABLE ▁W AX ING ▁AND ▁W AN ING | variable wax ##ing and wa ##ning |\n",
    "| TUMORAL INFILTRATION       |       2 | TUMORAL INFILTRATION       | TUMORAL INFILTRATION       | tumor ##al in ##filtration       | tumor ##al in ##fi ##ltration    | T ##UM ##OR ##AL IN ##FI ##LT ##RA ##TI ##ON    | tumor ##al in ##fil ##tration       | tumoral infiltration             | ▁T UM OR AL ▁IN FIL TR ATION         | tumoral infiltration             |\n",
    "| MARKEDLY REDUCED           |       2 | MARKEDLY REDUCED           | MARKEDLY REDUCED           | markedly reduced                 | marked ##ly reduced              | MA ##R ##KE ##D ##L ##Y R ##ED ##UC ##ED        | marked ##ly reduced                 | markedly reduced                 | ▁M ARK ED LY ▁R ED UC ED             | markedly reduced                 |\n",
    "| hyper - inflated           |       2 | hyper - inflated           | hyper - inflated           | hyper - inflated                 | h ##yper - in ##f ##lated        | h ##yper - in ##f ##lated                       | hy ##per - in ##f ##lated           | hyper - inflated                 | ▁hyper ▁- ▁infl ated                 | hyper - infl ##ated              |\n",
    "| FOCAL BULLAE               |       1 | FOCAL BULLAE               | FOCAL BULLAE               | focal bull ##ae                  | focal bull ##ae                  | F ##OC ##AL B ##U ##LL ##A ##E                  | focal bu ##lla ##e                  | focal bull ##ae                  | ▁F OC AL ▁B ULL AE                   | focal bull ##ae                  |\n",
    "| RIGHT ASCENDING AORTA      |       1 | RIGHT ASCENDING AORTA      | RIGHT ASCENDING AORTA      | right ascending ao ##rta         | right ascending a ##ort ##a      | R ##IG ##HT AS ##CE ##ND ##ING A ##OR ##TA      | right as ##cend ##ing ao ##rta      | right ascending aorta            | ▁R IGHT ▁A SC END ING ▁A ORT A       | right ascending aorta            |\n",
    "| DISSECTING AIR             |       1 | DISSECTING AIR             | DISSECTING AIR             | di ##sse ##cting air             | di ##sse ##cting air             | D ##IS ##SE ##CT ##ING AI ##R                   | disse ##cting air                   | dissecting air                   | ▁DIS SE CT ING ▁A IR                 | dissecting air                   |\n",
    "| ACHALASIA                  |       1 | ACH AL ASIA                | ACH ##AL ##ASIA            | ac ##hala ##sia                  | a ##chal ##asi ##a               | AC ##HA ##LA ##SI ##A                           | ach ##alas ##ia                     | ach ##ala ##si ##a               | ▁A CH AL AS IA                       | ach ##ala ##si ##a               |\n",
    "| NEOPLASTIC FOCUS           |       1 | NEOPLASTIC FOCUS           | NEOPLASTIC FOCUS           | neo ##pl ##astic focus           | neo ##p ##lastic focus           | NE ##OP ##LA ##ST ##IC F ##OC ##US              | neo ##pla ##stic focus              | neoplastic focus                 | ▁NE OP LAST IC ▁F OC US              | neoplastic focus                 |\n",
    "\n",
    "\n",
    "### **Insights**\n",
    "- Common terms like \"RIGHT\" and \"PLEURAL\" are mostly preserved across tokenization schemes, though some pretrained tokenizers split them into subwords.\n",
    "- Rare or domain-specific terms (e.g., \"hyper - inflated\") are frequently split, potentially affecting downstream task performance.\n",
    "- Despite tokenization challenges, pretrained models like Bio_ClinicalBERT are robust and can adapt well during fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "| Token                      |   Count | BPE                        | WordPiece                  | DistilBERT                       | Bio_ClinicalBERT                 | MedBERT                                         | ClinicalBERT                        | SapBERT                          | BioMistral                           | BiomedCLIP                       |\n",
      "|:---------------------------|--------:|:---------------------------|:---------------------------|:---------------------------------|:---------------------------------|:------------------------------------------------|:------------------------------------|:---------------------------------|:-------------------------------------|:---------------------------------|\n",
      "| RIGHT                      |  133060 | RIGHT                      | RIGHT                      | right                            | right                            | R ##IG ##HT                                     | right                               | right                            | ▁R IGHT                              | right                            |\n",
      "| LEFT                       |  127717 | LEFT                       | LEFT                       | left                             | left                             | L ##EF ##T                                      | left                                | left                             | ▁LE FT                               | left                             |\n",
      "| PLEURAL                    |  115285 | PLEURAL                    | PLEURAL                    | pl ##eur ##al                    | p ##le ##ural                    | P ##LE ##UR ##AL                                | pl ##eur ##al                       | pleural                          | ▁P LE UR AL                          | pleural                          |\n",
      "| STABLE                     |   85669 | STABLE                     | STABLE                     | stable                           | stable                           | ST ##AB ##LE                                    | stable                              | stable                           | ▁ST ABLE                             | stable                           |\n",
      "| EFFUSION                   |   84829 | EFFUSION                   | EFFUSION                   | e ##ff ##usion                   | e ##ff ##usion                   | E ##FF ##US ##ION                               | ef ##fus ##ion                      | effusion                         | ▁E FF US ION                         | effusion                         |\n",
      "| LUNG                       |   83813 | LUNG                       | LUNG                       | lung                             | lung                             | L ##UN ##G                                      | lung                                | lung                             | ▁L UN G                              | lung                             |\n",
      "| PULMONARY                  |   83238 | PULMONARY                  | PULMONARY                  | pulmonary                        | pulmonary                        | P ##U ##LM ##ON ##AR ##Y                        | pu ##lm ##onar ##y                  | pulmonary                        | ▁P UL MON ARY                        | pulmonary                        |\n",
      "| right                      |   78237 | right                      | right                      | right                            | right                            | right                                           | right                               | right                            | ▁right                               | right                            |\n",
      "| EDEMA                      |   74638 | EDEMA                      | EDEMA                      | ed ##ema                         | ed ##ema                         | E ##DE ##MA                                     | ede ##ma                            | edema                            | ▁E DE MA                             | edema                            |\n",
      "| left                       |   71237 | left                       | left                       | left                             | left                             | left                                            | left                                | left                             | ▁left                                | left                             |\n",
      "| PNEUMOTHORAX               |   66651 | PNEUMOTHORAX               | PNEUMOTHORAX               | p ##ne ##um ##otho ##ra ##x      | p ##ne ##um ##oth ##orax         | P ##NE ##UM ##OT ##H ##OR ##A ##X               | pne ##umo ##th ##orax               | pneumothorax                     | ▁P NE UM OT H OR AX                  | pneumothorax                     |\n",
      "| pleural                    |   65192 | pleural                    | pleural                    | pl ##eur ##al                    | p ##le ##ural                    | p ##le ##ural                                   | pl ##eur ##al                       | pleural                          | ▁ple ural                            | pleural                          |\n",
      "| UNCHANGED                  |   60725 | UNCHANGED                  | UNCHANGED                  | unchanged                        | unchanged                        | UN ##CH ##AN ##GE ##D                           | un ##chang ##ed                     | unchanged                        | ▁UN CHAN G ED                        | unchanged                        |\n",
      "| ATELECTASIS                |   58278 | ATELECTASIS                | ATELECTASIS                | ate ##le ##cta ##sis             | ate ##lect ##asis                | AT ##EL ##EC ##TA ##SI ##S                      | at ##ele ##cta ##sis                | ate ##lec ##ta ##sis             | ▁A TE LECT AS IS                     | ate ##le ##ct ##asis             |\n",
      "| CONSOLIDATION              |   53031 | CONSOLIDATION              | CONSOLIDATION              | consolidation                    | consolidation                    | CO ##NS ##OL ##ID ##AT ##ION                    | con ##sol ##idat ##ion              | consolidation                    | ▁CON S OL ID ATION                   | consolidation                    |\n",
      "| MOST PROMINENTLY           |       2 | MOST PROMINENTLY           | MOST PROMINENTLY           | most prominently                 | most prominently                 | M ##OS ##T PR ##OM ##IN ##EN ##TL ##Y           | most prominent ##ly                 | most prominently                 | ▁MO ST ▁P ROM IN ENT LY              | most prominently                 |\n",
      "| VARIABLE WAXING AND WANING |       2 | VARIABLE WAXING AND WANING | VARIABLE WAXING AND WANING | variable wax ##ing and wan ##ing | variable wax ##ing and wa ##ning | VA ##RI ##AB ##LE WA ##X ##ING AND WA ##NI ##NG | variable wa ##xin ##g and wa ##ning | variable wax ##ing and wan ##ing | ▁VAR I ABLE ▁W AX ING ▁AND ▁W AN ING | variable wax ##ing and wa ##ning |\n",
      "| TUMORAL INFILTRATION       |       2 | TUMORAL INFILTRATION       | TUMORAL INFILTRATION       | tumor ##al in ##filtration       | tumor ##al in ##fi ##ltration    | T ##UM ##OR ##AL IN ##FI ##LT ##RA ##TI ##ON    | tumor ##al in ##fil ##tration       | tumoral infiltration             | ▁T UM OR AL ▁IN FIL TR ATION         | tumoral infiltration             |\n",
      "| MARKEDLY REDUCED           |       2 | MARKEDLY REDUCED           | MARKEDLY REDUCED           | markedly reduced                 | marked ##ly reduced              | MA ##R ##KE ##D ##L ##Y R ##ED ##UC ##ED        | marked ##ly reduced                 | markedly reduced                 | ▁M ARK ED LY ▁R ED UC ED             | markedly reduced                 |\n",
      "| hyper - inflated           |       2 | hyper - inflated           | hyper - inflated           | hyper - inflated                 | h ##yper - in ##f ##lated        | h ##yper - in ##f ##lated                       | hy ##per - in ##f ##lated           | hyper - inflated                 | ▁hyper ▁- ▁infl ated                 | hyper - infl ##ated              |\n",
      "| FOCAL BULLAE               |       1 | FOCAL BULLAE               | FOCAL BULLAE               | focal bull ##ae                  | focal bull ##ae                  | F ##OC ##AL B ##U ##LL ##A ##E                  | focal bu ##lla ##e                  | focal bull ##ae                  | ▁F OC AL ▁B ULL AE                   | focal bull ##ae                  |\n",
      "| RIGHT ASCENDING AORTA      |       1 | RIGHT ASCENDING AORTA      | RIGHT ASCENDING AORTA      | right ascending ao ##rta         | right ascending a ##ort ##a      | R ##IG ##HT AS ##CE ##ND ##ING A ##OR ##TA      | right as ##cend ##ing ao ##rta      | right ascending aorta            | ▁R IGHT ▁A SC END ING ▁A ORT A       | right ascending aorta            |\n",
      "| DISSECTING AIR             |       1 | DISSECTING AIR             | DISSECTING AIR             | di ##sse ##cting air             | di ##sse ##cting air             | D ##IS ##SE ##CT ##ING AI ##R                   | disse ##cting air                   | dissecting air                   | ▁DIS SE CT ING ▁A IR                 | dissecting air                   |\n",
      "| ACHALASIA                  |       1 | ACH AL ASIA                | ACH ##AL ##ASIA            | ac ##hala ##sia                  | a ##chal ##asi ##a               | AC ##HA ##LA ##SI ##A                           | ach ##alas ##ia                     | ach ##ala ##si ##a               | ▁A CH AL AS IA                       | ach ##ala ##si ##a               |\n",
      "| NEOPLASTIC FOCUS           |       1 | NEOPLASTIC FOCUS           | NEOPLASTIC FOCUS           | neo ##pl ##astic focus           | neo ##p ##lastic focus           | NE ##OP ##LA ##ST ##IC F ##OC ##US              | neo ##pla ##stic focus              | neoplastic focus                 | ▁NE OP LAST IC ▁F OC US              | neoplastic focus                 |\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add more pretrained tokenizers\n",
    "pretrained_models = {\n",
    "    \"DistilBERT\": \"distilbert-base-uncased\",\n",
    "    \"Bio_ClinicalBERT\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "    \"MedBERT\": \"Charangan/MedBERT\",\n",
    "    \"ClinicalBERT\": \"medicalai/ClinicalBERT\",\n",
    "    \"SapBERT\": \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\",\n",
    "    \"BioMistral\" : \"BioMistral/BioMistral-7B\",\n",
    "    \"BiomedCLIP\": \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    "}\n",
    "\n",
    "examples = [\n",
    "    \"PICC line insertion noted with cardiomediastinal shift.\",\n",
    "    \"Retrocardiac opacity observed in the lower lobe.\",\n",
    "    \"Atelectasis seen in the left lung base.\"\n",
    "]\n",
    "# Combine top 15, middle 5, and bottom 5 tokens\n",
    "middle_index = len(sorted_tokens_by_count) // 2\n",
    "sampled_tokens_with_counts = (\n",
    "    sorted_tokens_by_count[:15] +  # Top 15\n",
    "    sorted_tokens_by_count[middle_index:middle_index + 5] +  # Middle 5\n",
    "    sorted_tokens_by_count[-5:]  # Bottom 5\n",
    ")\n",
    "\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "# Prepare corpus from RadGraph\n",
    "def load_radgraph_text(file_path):\n",
    "    \"\"\"Load text from RadGraph reports for training tokenizers.\"\"\"\n",
    "    # Example implementation (adjust based on actual RadGraph format)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        reports = f.readlines()\n",
    "    return reports\n",
    "\n",
    "# Load the corpus\n",
    "corpus = load_radgraph_text(\"../data/radgraph_reports.txt\")\n",
    "\n",
    "# Train BPE tokenizer\n",
    "bpe_tokenizer = Tokenizer(models.BPE())\n",
    "bpe_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "bpe_trainer = trainers.BpeTrainer(vocab_size=30000, min_frequency=2)\n",
    "bpe_tokenizer.train_from_iterator(corpus, trainer=bpe_trainer)\n",
    "\n",
    "# Train WordPiece tokenizer\n",
    "wp_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "wp_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "wp_trainer = trainers.WordPieceTrainer(vocab_size=30000, special_tokens=[\"[UNK]\"])\n",
    "wp_tokenizer.train_from_iterator(corpus, trainer=wp_trainer)\n",
    "\n",
    "# Save tokenizers in the notebook directory\n",
    "os.makedirs(\"models\", exist_ok=True)  # Save relative to the notebook location\n",
    "bpe_tokenizer.save(\"models/bpe_tokenizer.json\")\n",
    "wp_tokenizer.save(\"models/wp_tokenizer.json\")\n",
    "\n",
    "# Tokenize sampled tokens\n",
    "comparison_results = []\n",
    "for token, count in sampled_tokens_with_counts:\n",
    "    row = {\"Token\": token, \"Count\": count}\n",
    "    row[\"BPE\"] = \" \".join(bpe_tokenizer.encode(token).tokens)\n",
    "    row[\"WordPiece\"] = \" \".join(wp_tokenizer.encode(token).tokens)\n",
    "    for model_name, model_path in pretrained_models.items():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        row[model_name] = \" \".join(tokenizer.tokenize(token))\n",
    "    comparison_results.append(row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "# Display as Markdown\n",
    "print(comparison_df.to_markdown(index=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Conclusions and Next Steps\n",
    "\n",
    "### **Takeaways**\n",
    "1. **Corpus Analysis**:\n",
    "   - RadGraph’s token distribution highlights significant repetition among anatomical and diagnostic terms.\n",
    "   - Rare or domain-specific terms could benefit from improved tokenization.\n",
    "\n",
    "2. **Tokenization Insights**:\n",
    "   - Pretrained tokenizers handle common terms well but often split rare phrases into subwords.\n",
    "   - While custom tokenization could improve representation for domain-specific terms, pretrained models are likely sufficient for most tasks.\n",
    "\n",
    "### **Next Steps**\n",
    "- Fine-tune pretrained models (e.g., BioBERT, ClinicalBERT) on RadGraph for tasks like NER and relation extraction.\n",
    "- Evaluate the impact of token splits on model performance.\n",
    "- Explore custom tokenization schemes as a future experiment.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
